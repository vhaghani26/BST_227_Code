{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleLatentSequenceModel.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sMY2rA5rT4UC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhaghani26/BST_227_Code/blob/main/ground_truth_motif_locations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbV2qTzTTuiL"
      },
      "source": [
        "# *Utility function for reading in data from course Box folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFWtnpM72GJW"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#----------------------------------------\n",
        "#from Chenxi Liu\n",
        "#function for downloading sequence data from Box and converting to one-hot encoding\n",
        "#note this function actually returns the summary statistics over the data, which is\n",
        "#just the total number of each bases across all input sequences for our simple \n",
        "#latent variable model.\n",
        "#\n",
        "# this function by default returns a summary of all data.\n",
        "def get_sequence(url, categories=['A', 'C', 'G', 'T']):\n",
        "  r = requests.get(url)\n",
        "  df = pd.read_csv(io.StringIO(r.text), sep=\" \", header=None)\n",
        "  s1 = np.array(list(str(df.to_numpy()[0, :][0])), dtype=object).reshape(-1, 1)\n",
        "\n",
        "  m = len(df)\n",
        "  sequence_len = len(list(df.iloc[0, :].values)[0])\n",
        "  data = np.zeros((m, sequence_len, len(categories)))\n",
        "  data_ss = np.zeros((1, len(categories)))\n",
        "\n",
        "  ohe = OneHotEncoder(sparse=False, categories=[np.array(categories, dtype=object)])\n",
        "  example_sequence = np.array(list(str(df.to_numpy()[0, :][0])), dtype=object).reshape(-1, 1)\n",
        "\n",
        "  ohe.fit(s1)\n",
        "\n",
        "  for ii in tqdm(range(m)):\n",
        "    \n",
        "    s = list(str(df.to_numpy()[ii, :][0]))\n",
        "    s_a = np.array(s).reshape(-1, 1)\n",
        "    data[ii, :, :] = ohe.transform(s_a)\n",
        "  \n",
        "  for ii in range(len(categories)):\n",
        "    data_ss[0,ii] = np.sum(data[:,:,ii])\n",
        "\n",
        "  return data, data_ss\n",
        "\n",
        "#----------------------------------------\n",
        "#modification of function above, to return a 'training' and 'testing' set (randomly split).\n",
        "def get_sequence_traintest(url, categories=['A', 'C', 'G', 'T'], FRACTION_TRAINING=0.8):\n",
        "  r = requests.get(url)\n",
        "  df = pd.read_csv(io.StringIO(r.text), sep=\" \", header=None)\n",
        "  s1 = np.array(list(str(df.to_numpy()[0, :][0])), dtype=object).reshape(-1, 1)\n",
        "\n",
        "  m = len(df)\n",
        "  sequence_len = len(list(df.iloc[0, :].values)[0])\n",
        "  data = np.zeros((m, sequence_len, len(categories)))\n",
        "  data_ss_train = np.zeros((1, len(categories)))\n",
        "  data_ss_test = np.zeros((1, len(categories)))\n",
        "\n",
        "  ohe = OneHotEncoder(sparse=False, categories=[np.array(categories, dtype=object)])\n",
        "  example_sequence = np.array(list(str(df.to_numpy()[0, :][0])), dtype=object).reshape(-1, 1)\n",
        "\n",
        "  ohe.fit(s1)\n",
        "\n",
        "  for ii in tqdm(range(m)):\n",
        "    \n",
        "    s = list(str(df.to_numpy()[ii, :][0]))\n",
        "    s_a = np.array(s).reshape(-1, 1)\n",
        "    data[ii, :, :] = ohe.transform(s_a)\n",
        "\n",
        "\n",
        "   #randomly permute rows of matrix\n",
        "  np.random.shuffle(data)\n",
        "\n",
        "  train_indices = np.arange(start=0,stop=round(FRACTION_TRAINING*data.shape[0]))\n",
        "  test_indices = np.arange(start=round(FRACTION_TRAINING*data.shape[0]), stop=data.shape[0])\n",
        "  \n",
        "  for ii in range(len(categories)):\n",
        "    data_ss_train[0,ii] = np.sum(data[train_indices,:,ii])\n",
        "    data_ss_test[0,ii] = np.sum(data[test_indices,:,ii])\n",
        "\n",
        "  return data_ss_train, data_ss_test\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMY2rA5rT4UC"
      },
      "source": [
        "# Core EM code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Dsy_AdUbzE"
      },
      "source": [
        "#----------------------------------------\n",
        "# parameter initialization\n",
        "# psi: a 4xNUM_MODELS matrix of psi parameters\n",
        "# lmbda: an array of length NUM_MODELS\n",
        "\n",
        "import math\n",
        "\n",
        "def init_EM(NUM_MODELS):\n",
        "  lmbda = np.random.uniform(0, 1,size=(NUM_MODELS,))\n",
        "  lmbda = lmbda/np.sum(lmbda)\n",
        "  psi = np.random.uniform(0,1,size=(4,NUM_MODELS))\n",
        "  psi = psi/psi.sum(axis=0)\n",
        "  theta = {'lmbda': lmbda, 'psi': psi}\n",
        "  return(theta)\n",
        "\n",
        "#----------------------------------------\n",
        "# compute posteriors P(C_ij | X_ij, theta)\n",
        "# return a 4 x NUM_MODELS object PP, where PP[aa,bb] = P(C_ij = bb | X_ij = aa, theta)\n",
        "def E_step(theta):\n",
        "  unnormalized_posteriors = theta['psi']*theta['lmbda']\n",
        "  normalized_posteriors = (unnormalized_posteriors.T/unnormalized_posteriors.sum(axis=1)).T\n",
        "  return(normalized_posteriors)\n",
        "\n",
        "#----------------------------------------\n",
        "# compute MLE of psi, lambda\n",
        "# theta: current parameter set used to calculate posteriors\n",
        "# XXss: summary statistics of # of bases across all training sequences\n",
        "# return a dictionary containing psi, lambda\n",
        "def M_step(XXss, posteriors):\n",
        "  unnormalized_psi = (posteriors.T*XXss).T\n",
        "  unnormalized_lambda = unnormalized_psi.sum(axis=0)\n",
        "  psi = unnormalized_psi/unnormalized_psi.sum(axis=0)\n",
        "  lmbda = unnormalized_lambda/np.sum(unnormalized_lambda)\n",
        "  return({'lmbda': lmbda, 'psi': psi})\n",
        "\n",
        "#----------------------------------------\n",
        "# compute log likelihood, given posteriors based off the current parameter set theta (so KL = 0)\n",
        "def loglikelihood(XXss, theta, posteriors):\n",
        "  #matrix JP of weighted joint log probabilities, size 4xNUM_MODELS, where JP[aa,bb] = E[Cij=bb] log P(Xij = aa, Cij = bb | theta) \n",
        "  weighted_log_joint_prob = (np.log(theta['psi'])+np.log(theta['lmbda']))*posteriors  \n",
        "  weighted_log_joint_prob[np.where(posteriors == 0)]=0; #putting in a check in case some posteriors are 0 (because some psi are 0) - 0log0 = 0 (otherwise we will get nan)\n",
        "  \n",
        "  expected_complete_LL = np.sum(weighted_log_joint_prob.T*XXss)\n",
        "  #entropy term calculation needs to be careful since 0log0 = 0, but coded naively will give nan\n",
        "  qlogq = posteriors * np.log(posteriors);\n",
        "  qlogq[np.where(posteriors == 0)] = 0 #0log0 = 0\n",
        "  return (expected_complete_LL - np.sum(np.sum(qlogq,axis=1) * XXss)) #log likelihood = ELBO + entropy, when q=p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntT0C1NLUmwZ"
      },
      "source": [
        "# Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLRHXHZF2NYA",
        "outputId": "3f1f965f-2ec4-4e17-e143-1546232c36c7"
      },
      "source": [
        "#URL for at_gc_sequences.txt - this is a single sequence:\n",
        "# ATTTAATATAAAATTTGGCCGCCATAAAAAAA\n",
        "#https://ucdavis.box.com/shared/static/s8g6zx9vwxbbfdxdj2uqzhlvslc1jhsy.txt\n",
        "#URL for sequence.padded.txt - the real binding site data\n",
        "#https://ucdavis.box.com/shared/static/0cacx2xvn4ugxo9h21ci2ngesryigf43.txt\n",
        "#URL for sequence.motiflocation.padded.txt - the location of the binding sites from sequence.padded.txt\n",
        "#https://ucdavis.box.com/shared/static/gd0r12mdkhix86bo9ffbn3dy0fy0prmn.txt\n",
        "\n",
        "#sequences_padded_train, sequences_padded_test = get_sequence_traintest('https://ucdavis.box.com/shared/static/0cacx2xvn4ugxo9h21ci2ngesryigf43.txt')\n",
        "#XXss_at_gc, XX_sequences_padded = get_sequence('https://ucdavis.box.com/shared/static/s8g6zx9vwxbbfdxdj2uqzhlvslc1jhsy.txt')\n",
        "XXss_sequences_padded, XX_sequences_padded = get_sequence('https://ucdavis.box.com/shared/static/0cacx2xvn4ugxo9h21ci2ngesryigf43.txt')\n",
        "\n",
        "#get the locations of the binding sites\n",
        "CC_sequences_padded = pd.read_csv(io.StringIO(requests.get('https://ucdavis.box.com/shared/static/gd0r12mdkhix86bo9ffbn3dy0fy0prmn.txt').text), sep=\",\", header=None).to_numpy()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 357/357 [00:00<00:00, 2177.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp1t6QtSUqTk"
      },
      "source": [
        "# Run EM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJTn2LVl3yco",
        "outputId": "45683a7c-9a6d-43e3-84a4-fcce4f3e574a"
      },
      "source": [
        "np.random.seed(1)\n",
        "#XXss = XXss_at_gc\n",
        "XXss = sequences_padded_train\n",
        "theta = init_EM(NUM_MODELS = 3)\n",
        "\n",
        "#train\n",
        "for ii in range(3):  \n",
        "  posteriors = E_step(theta);  \n",
        "  print(loglikelihood(XXss, theta, posteriors))\n",
        "  theta = M_step(XXss, posteriors);\n",
        "\n",
        "#evaluate on held-out test data\n",
        "posteriors = E_step(theta)\n",
        "print('held-out likelihood:', loglikelihood(sequences_padded_test, theta, posteriors))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-17226.31079541078\n",
            "-15061.901049117349\n",
            "-15061.901049117349\n",
            "held-out likelihood: -3739.454646466645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iL4qXdg77fw"
      },
      "source": [
        "\n",
        "\n",
        "# Some exercises to do\n",
        "\n",
        "1. Try different random initializations. How do the final log likelihoods compare? How do the psi's and lambdas compare? (e.g. how many unique solutions did you find, and how were their corresponding likelihoods?) Explain your observations. What can we conclude about the utility of such a model?\n",
        "\n",
        "2. Train the model using the dataset \"XXss_at_gc\", which is just a dataset consisting of a single sequence, \"ATTTAATATAAAATTTGGCCGCCATAAAAAAA\". Looking carefully, this is just a GC-rich sequence, flanked by AT-rich sequences. What do you expect the two models to learn? Try training it now, and look at the psi's. Did the model learn what you expected? Why or why not?\n"
      ]
    }
  ]
}