{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxCPwL7tf9QNmWHnebZNiq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhaghani26/BST_227_Code/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9zcb_cbvagI"
      },
      "source": [
        "# **Assignment 2**\n",
        "By: Mariele Lensink and Viktoria Haghani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Kv0WNn6hn0"
      },
      "source": [
        "It is possible to fix both limitations at once, but the model will be more complex than we have time to spend on it. So instead, we will fix only the limitation that $\\lambda_{j}$ cannot vary by changing the model such that each sequence gets their own $\\lambda$ variables: in other words, making $\\lambda$ a latent variable instead of a parameter, and giving each sequence its own lambda $\\vec{\\lambda_{i}}$:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdpr9JcE75Dl"
      },
      "source": [
        "*   $\\psi^{(l)}_{kp}$: A matrix of parameters of size 4 x *P*, where *P* is a hyperparameter representing the length of a motif (e.g. *P* = 6 or *P* = 8 might be common choices). *l* = 0 or *l* = 1.\n",
        "*   $X_{ijp}$: The base at position *p* of the subsequence starting at position *j* of sequence *i*\n",
        "*   $X_{ijpk}$: A dummy indicator for which $X_{ijpk} = 1$ iff $X_{ijp} = k$, otherwise 0.\n",
        "*   $C_{i} \\in \\{1, ..., L - P + 1\\}$: A latent variable indicating the location of the motif in sequence *i*\n",
        "*   $\\vec{\\lambda_{i}}$: A vector of probabilities, where $P(C_{ij} = 1 | \\theta) = \\lambda_{ij}$\n",
        "*   $P(\\vec{\\lambda_{i}}|\\vec{\\alpha})=Dir(\\vec{\\alpha})$, where $Dir(\\cdot)$ is the Dirichlet distribution, from which you can sample vectors of probabilities\n",
        "*   $P(C_{i}=j|\\theta)=\\lambda_{ij}$\n",
        "*   $P(X_{ijp}=k|C_{i},\\theta)=\\psi^{(C_{ij})}_{pk}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8gAC6U4-apz"
      },
      "source": [
        "The complete likelihood is:\n",
        "\n",
        "\\begin{equation}\n",
        "P(X,\\{\\vec{\\lambda_{i}}\\},C|\\theta)= \\prod_{i}P(\\vec{\\lambda_{i}}|\\vec{\\alpha})P(C_{i}|\\lambda_{i})\\prod_{j}P(X_{ij}|C_{ij},\\theta)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "= \\prod_{i} B^{-1}(\\alpha)[\\prod_{t}\\lambda^{\\alpha_{t}}_{it}]\\prod_{j}\\lambda^{C_{ij}}_{ij}\\prod_{p}\\prod_{k}[\\psi^{(1)}_{pk}]^{X_{ijpk}C_{ij}}[\\psi^{(0)}_{pk}]^{X_{ijpk}(1-C_{ij})}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n88PfSTKBXI9"
      },
      "source": [
        "You will notice for this model that $\\vec{\\lambda_{i}}$ has become a latent variable, and not just a parameter to estimate now. The problem we run into now is that the EM algorithm requires us to compute posteriors over the latent variables, which is now the set $\\{C_{ij},\\vec{\\lambda_{i}}\\}$. In the complete likelihood function defined above, you can see the $C_{ij}$ depend on the\n",
        "$\\vec{\\lambda_{i}}$, and so we have:\n",
        "\n",
        "\\begin{equation}\n",
        "P(C,\\{\\vec{\\lambda_{i}}\\}|X,\\theta)=\\prod_{i}P(C_{i},\\vec{\\lambda_{i}}|X_{i}, \\theta)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yzpXBFLB3HD"
      },
      "source": [
        "So to use the EM algorithm, we have to compute the individual posteriors $P(C_{i},\\vec{\\lambda_{i}}|X_{i}, \\theta)$, which goes as follows:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiboCUGXCc0w"
      },
      "source": [
        "\\begin{equation}\n",
        "P(C_{i},\\vec{\\lambda_{i}}|X_{i}, \\theta) = \\frac{P(C_{i},\\vec{\\lambda_{i}}, X_{i}|\\theta)}{P(X_{i}|\\theta)}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "P(C_{i},\\vec{\\lambda_{i}}|X_{i}, \\theta) = \\frac{P(\\vec{\\lambda_{i}}|\\vec{\\alpha})P(C_{i}|\\vec{\\lambda_{i}})P(X_{i}|C_{i})}{P(X_{i}|\\theta)}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "P(C_{i},\\vec{\\lambda_{i}}|X_{i}, \\theta) \\propto P(\\vec{\\lambda_{i}}|\\vec{\\alpha})P(C_{i}|\\vec{\\lambda_{i}})P(X_{i}|C_{i})\n",
        "\\end{equation}"
      ]
    }
  ]
}