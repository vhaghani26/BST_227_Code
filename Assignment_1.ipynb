{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhaghani26/BST_227_Code/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEB6Jbmxr9P7"
      },
      "source": [
        "# **The EM Algorithm for a more complex sequence model**\n",
        "## By: Mariele Lensink and Viktoria Haghani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zDlf51Mr6wp"
      },
      "source": [
        "First, we need to import the modules we plan to use to run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yrbLTPNtux7"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tqdm import tqdm\n",
        "import math"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4CTnzFwsnzw"
      },
      "source": [
        "# **Core EM Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plTuQuU6rxDV"
      },
      "source": [
        "This section includes all the functions used in the EM implementation, including data preparation, random parameter initialization, the E-step, the M-step, and the log likelihood calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdktorRL4uEj"
      },
      "source": [
        "Write a function to download and one-hot encoded the sequence data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka8lwgcXZ5XF"
      },
      "source": [
        "# Function written by Chenxi Liu (slight modifications made)\n",
        "\n",
        "def get_sequence(url, categories=['A', 'C', 'G', 'T']):\n",
        "  # Send a GET request to a specified URL, categories=['A', 'C', 'G', 'T']):\n",
        "  r = requests.get(url)\n",
        "  # Convert sequence data to data frame\n",
        "  df = pd.read_csv(io.StringIO(r.text), sep=\" \", header=None)\n",
        "  # Turn the first sequence into a 2D array where each index is an independent array with a single base pair\n",
        "  s1 = np.array(list(str(df.to_numpy()[0, :][0])), dtype=object).reshape(-1, 1)\n",
        "\n",
        "  # Determine how many sequences there are in the text file\n",
        "  num_seqs = len(df)\n",
        "  # Assume all input sequences are equal length\n",
        "  # Determine how long each sequence is\n",
        "  seq_len = len(list(df.iloc[0, :].values)[0])\n",
        "  # Start to make a one-hot encoded 3D matrix for the seqeunces\n",
        "  # Not one-hot encoded yet, just zeroes \n",
        "  data = np.zeros((num_seqs, seq_len, len(categories)))\n",
        "  # Make a matrix repersenting each category ['A', 'C', 'G', 'T']\n",
        "  bp_counts = np.zeros((1, len(categories)))\n",
        "\n",
        "  # Encode categorical feautures in a one-hot numeric array\n",
        "  # Assign categories to be used\n",
        "  ohe = OneHotEncoder(sparse=False, categories=[np.array(categories, dtype=object)])\n",
        "  # Apply OneHotEncoder to example sequence\n",
        "  ohe.fit(s1)\n",
        "  \n",
        "  # Apply OneHotEncoder to all sequences\n",
        "  for ii in tqdm(range(num_seqs)):\n",
        "    s = list(str(df.to_numpy()[ii, :][0]))\n",
        "    s_a = np.array(s).reshape(-1, 1)\n",
        "    data[ii, :, :] = ohe.transform(s_a)\n",
        "  \n",
        "  # Count the number of each base in the sequence data\n",
        "  for ii in range(len(categories)):\n",
        "    bp_counts[0,ii] = np.sum(data[:,:,ii])\n",
        "  \n",
        "  # Return the one-hot encoded matrix (data),\n",
        "  # the counts for each base pair (bp_counts),\n",
        "  # the number of sequences (N),\n",
        "  # and the length of each sequence (L)\n",
        "  return data, bp_counts                                                  "
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od2WQCQrDDyv"
      },
      "source": [
        "Write a modified version of the above function that returns a randomly split training and test data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJGr38gKDNuE"
      },
      "source": [
        "# Function written by Chenxi Liu and Gerald Quon\n",
        "\n",
        "def get_sequence_traintest(url, categories=['A', 'C', 'G', 'T'], FRACTION_TRAINING=0.8):\n",
        "  # Send a GET request to a specified URL, categories=['A', 'C', 'G', 'T']):\n",
        "  r = requests.get(url)\n",
        "  # Convert sequence data to data frame\n",
        "  df = pd.read_csv(io.StringIO(r.text), sep=\" \", header=None)\n",
        "  # Turn the first sequence into a 2D array where each index is an independent array with a single base pair\n",
        "  s1 = np.array(list(str(df.to_numpy()[0, :][0])), dtype=object).reshape(-1, 1)\n",
        "\n",
        "  # Determine how many sequences there are in the text file\n",
        "  m = len(df)\n",
        "  # Assume all input sequences are equal length\n",
        "  # Determine how long each sequence is\n",
        "  sequence_len = len(list(df.iloc[0, :].values)[0])\n",
        "  # Start to make a one-hot encoded 3D matrix for the seqeunces\n",
        "  # Not one-hot encoded yet, just zeroes \n",
        "  data = np.zeros((m, sequence_len, len(categories)))\n",
        "  # Make a matrix repersenting each category ['A', 'C', 'G', 'T'] for the training set\n",
        "  bp_counts_train = np.zeros((1, len(categories)))\n",
        "  # Make a matrix repersenting each category ['A', 'C', 'G', 'T'] for the test set\n",
        "  bp_counts_test = np.zeros((1, len(categories)))\n",
        "\n",
        "  # Encode categorical feautures in a one-hot numeric array\n",
        "  # Assign categories to be used\n",
        "  ohe = OneHotEncoder(sparse=False, categories=[np.array(categories, dtype=object)])\n",
        "  # Apply OneHotEncoder to example sequence\n",
        "  ohe.fit(s1)\n",
        "\n",
        "  # Apply OneHotEncoder to all sequences\n",
        "  for ii in tqdm(range(m)):\n",
        "    s = list(str(df.to_numpy()[ii, :][0]))\n",
        "    s_a = np.array(s).reshape(-1, 1)\n",
        "    data[ii, :, :] = ohe.transform(s_a)\n",
        "\n",
        "  # Randomly permute rows of matrix\n",
        "  np.random.shuffle(data)\n",
        "\n",
        "  # Split data into training and text data\n",
        "  train_indices = np.arange(start=0,stop=round(FRACTION_TRAINING*data.shape[0]))\n",
        "  test_indices = np.arange(start=round(FRACTION_TRAINING*data.shape[0]), stop=data.shape[0])\n",
        "  \n",
        "  # Count the number of each base in the test and train sets\n",
        "  for ii in range(len(categories)):\n",
        "    bp_counts_train[0,ii] = np.sum(data[train_indices,:,ii])\n",
        "    bp_counts_test[0,ii] = np.sum(data[test_indices,:,ii])\n",
        "  \n",
        "  # Return the base pair counts for the test and train sets\n",
        "  return bp_counts_train, bp_counts_test"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbAXV5YlM9p0"
      },
      "source": [
        "Write functions that will initialize our parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0uB6d0_M_b-"
      },
      "source": [
        "# Make a function to randomly generate L - P + 1 lambdas that sum to 1\n",
        "def initialize_lambda(sequence_length, motif_length):\n",
        "  # Make an empty list to represent our parameter lambda_j\n",
        "  lambda_j = []\n",
        "  # Given our sequence length and motif length, determine how many lambdas we need\n",
        "  lambdas = sequence_length - motif_length + 1\n",
        "  # Fill up our empty matrix with the right number of lambdas using random probabilities\n",
        "  for i in range(lambdas):\n",
        "    lambda_j.append(np.random.random_sample())\n",
        "  # Normalize probabilities by dividing each one by the sum\n",
        "  # Need to set sum_lambdas on the outside of the for loop so it isn't recalculated every time\n",
        "  sum_lambdas = sum(lambda_j)\n",
        "  for i, lmbda in zip(range(lambdas), lambda_j):\n",
        "    lambda_j[i] = (lmbda/sum_lambdas)\n",
        "  # Return our lambda_j parameter\n",
        "  return lambda_j"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-wAaxEgsUYD"
      },
      "source": [
        "# Make a function to randomly generate psi\n",
        "# Psi is a 2D matrix: 4 x P (P = motif-length) \n",
        "# Each set of 4 corresponds to ['A', 'C', 'G', 'T']\n",
        "def initialize_psi(motif_length):\n",
        "  # Make a 4 x P matrix of zeroes\n",
        "  psi = np.zeros((motif_length, 4), dtype = float)\n",
        "  # For each sub-matrix of 4, generate probabilities that sum to 1\n",
        "  for ind_i, i in zip(range(motif_length), psi):\n",
        "    psi[ind_i] = (np.random.dirichlet(np.ones(4),size=1))\n",
        "  # Return our psi parameter\n",
        "  return psi"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vrifajwsUoK"
      },
      "source": [
        "# Store all initialized parameters in a dictionary called 'params' (params = theta)\n",
        "def initialize_random_params(sequence_length, motif_length):\n",
        "    params = {'lambda_j': initialize_lambda(sequence_length, motif_length),\n",
        "              'psi0': initialize_psi(motif_length),\n",
        "              'psi1': initialize_psi(motif_length)\n",
        "              }\n",
        "    return params"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC-Qv1XpvDeI"
      },
      "source": [
        "Write a function to calculate the posterior for the **E-step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQxY3F0qvG_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30c1d88-ff11-49ae-b949-901f226e5fee"
      },
      "source": [
        "def E_step(params,ohe_matrix,sequence_length,motif_length):\n",
        "  posts = np.zeros((len(ohe_matrix),(sequence_length-motif_length)))\n",
        " #loop through every seqence in file \n",
        "  for i in range(len(ohe_matrix)):\n",
        "    #loop through \n",
        "    for j in range(sequence_length - motif_length):\n",
        "     #initialize C_ij \n",
        "      C_ij = np.log(params['lambda_j'][j])\n",
        "      #compute P(X_ij|Cij = 1)\n",
        "      for p in range(motif_length):\n",
        "        for k in range(4):\n",
        "          if ohe_matrix[i][j+p][k] != 0:\n",
        "            C_ij = C_ij + np.log(ohe_matrix[i][j+p][k]*params['psi1'][p][k])\n",
        "            posts[i][j] = C_ij\n",
        "      for jprime in range(sequence_length-motif_length):\n",
        "        if jprime == j:\n",
        "          continue \n",
        "        for p in range(motif_length):\n",
        "          for k in range(4):\n",
        "            if ohe_matrix[i][j+p][k] != 0:\n",
        "              C_ij = C_ij + np.log(ohe_matrix[i][j+p][k]*params['psi0'][p][k])\n",
        "              posts[i][j] = C_ij\n",
        "  posts1 = np.zeros((len(posts),len(posts[0])))\n",
        "  for i in range(len(posts)):\n",
        "    rowmin = posts[i].min()\n",
        "    for j in range(len(posts[i])):\n",
        "      posts1[i][j] = math.exp(posts[i][j] - rowmin)\n",
        "      #posts1[i][j] = exp(posts1[i][j])\n",
        "    rowsum = posts1[i].sum()\n",
        "    for j in range(len(posts1[i])):\n",
        "      posts1[i][j] = posts1[i][j]/rowsum\n",
        "  # Return posterior matrix\n",
        "  return posts1\n",
        "\n",
        "# Test the E-step\n",
        "posterior_matrix = E_step(params,my_ohe_at_gc_sequences,len(my_ohe_at_gc_sequences[0]),motif_length)\n",
        "posterior_matrix"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.16111849e-72, 9.60387695e-61, 7.61965951e-86, 4.94668022e-73,\n",
              "        4.18703050e-02, 2.85998072e-05, 2.24694000e-52, 1.39101640e-36,\n",
              "        3.39181572e-61, 1.56805237e-44, 1.46071835e-14, 9.58100345e-01,\n",
              "        7.49971027e-07, 3.16258639e-42]])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S05xOBVOvHvh"
      },
      "source": [
        "Write a function to do the calculations for the **M-step**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8xF_OTs58bA"
      },
      "source": [
        "We will start with making a practice posterior of random probabilities. This allows us to troubleshoot the M-step independent of the E-step. It does not get used in the EM algorithm implementation, but it's here for debugging purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m60-pmemrATK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9484505-78bf-4c1c-8259-54a885f4a487"
      },
      "source": [
        "# Make practice posterior to troubleshoot M-step\n",
        "# Run this after loading data in (data loading is in Run EM section)\n",
        "\n",
        "# Sequence data used\n",
        "seq_data = my_ohe_sequence_padded\n",
        "# Set the number of sequences\n",
        "num_seqs = len(seq_data)\n",
        "# Set the length of the sequences\n",
        "seq_len = len(seq_data[0])\n",
        "\n",
        "\n",
        "practice_posterior = []\n",
        "for i in range(num_seqs):\n",
        "  new_row = initialize_lambda(seq_len, motif_length)\n",
        "  practice_posterior.append(new_row)\n",
        "\n",
        "print(len(practice_posterior)) # Should be total number of sequences; 357 for sequence padded\n",
        "print(len(practice_posterior[0])) # Should be number of j's; 31 for sequence padded"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "357\n",
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGa58p1LvRHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f34b6f9b-4a4a-493c-c7e9-b9019c52a5f1"
      },
      "source": [
        "def M_step(onehot_matrix, posterior, motif_length): \n",
        "  # Make dictionary to store new parameters\n",
        "  new_params = {}\n",
        "\n",
        "  # Calculate new lambda_j\n",
        "  # Add every column of the posterior\n",
        "  column_sums = np.array(posterior).sum(axis = 0)\n",
        "  # Divide by the total number of sequences\n",
        "  new_lambda_j = (column_sums/(len(posterior))).tolist()\n",
        "  # Confirm that the sum of our new_lambda_j is 1\n",
        "  print(f'The sum of our new lambda_j is: {sum(new_lambda_j)}')\n",
        "  # Add new_lambda_j to new_params\n",
        "  new_params[\"lambda_j\"] = new_lambda_j\n",
        "\n",
        "  # Calculate new psi1\n",
        "  # Determine how many j positions we have in our input matrix\n",
        "  num_js = len(onehot_matrix[0]) - motif_length + 1\n",
        "  # Determine matrix dimensions and save our window sequences\n",
        "  X_ijmks = []\n",
        "  for i, ind_i in zip(onehot_matrix, range(len(onehot_matrix))):\n",
        "    # Iterate through the indeces for our lambda parameter\n",
        "    for ind_j in range(num_js):\n",
        "        # i[ind_j:ind_j+motif_length] is the base pair identities of the One-Hot encoded matrix within the window starting at j\n",
        "        X_ijmk = (i[ind_j:ind_j+motif_length]).tolist()\n",
        "        # Save each X_ijmk to the X_ijmks list for ease of access\n",
        "        X_ijmks.append(X_ijmk)\n",
        "  # Manipulate the posterior so it maches the X_ijmks list\n",
        "  manipulated_posterior = []\n",
        "  for i in posterior:\n",
        "    for j in i:\n",
        "      manipulated_posterior.append(j)\n",
        "  # Make an empty list numerators, that will contain all the X_ijmks multiplied by C_ij    \n",
        "  numerators = []\n",
        "  for ind_post, X_ijmk in zip(range(len(manipulated_posterior)), X_ijmks):\n",
        "    # Turn X_ijmk into an array\n",
        "    X_ijmk_array = np.array(X_ijmk)\n",
        "    # Multiply X_ijmk by the posterior (C_ij)\n",
        "    numerator = X_ijmk_array * manipulated_posterior[ind_post]\n",
        "    # Add the multiplied window sequences to the list \"numerators\"\n",
        "    numerators.append(numerator)\n",
        "  numerator_before_normalization = np.array(numerators).sum(axis = 0)\n",
        "  new_psi1 = np.divide(numerator_before_normalization, len(onehot_matrix))\n",
        "  # Verify that new_psi1 rows sum to 1\n",
        "  print('The sums of each row in our new psi1 are:')\n",
        "  for i in range(len(new_psi1)):\n",
        "     print(sum(new_psi1[i]))\n",
        "  # Add new_psi1 to new_params\n",
        "  new_params[\"psi1\"] = new_psi1.tolist()\n",
        "\n",
        "  # Calculate new psi0\n",
        "  # Determine how many j positions we have in our input matrix\n",
        "  num_js = len(onehot_matrix[0]) - motif_length + 1\n",
        "  # Determine matrix dimensions and save our window sequences\n",
        "  X_ijmks = []\n",
        "  for i, ind_i in zip(onehot_matrix, range(len(onehot_matrix))):\n",
        "    # Iterate through the indeces for our lambda parameter\n",
        "    for ind_j in range(num_js):\n",
        "        # i[ind_j:ind_j+motif_length] is the base pair identities of the One-Hot encoded matrix within the window starting at j\n",
        "        X_ijmk = (i[ind_j:ind_j+motif_length]).tolist()\n",
        "        # Save each X_ijmk to the X_ijmks list for ease of access\n",
        "        X_ijmks.append(X_ijmk)\n",
        "  # Manipulate the posterior so it maches the X_ijmks list\n",
        "  manipulated_posterior = []\n",
        "  for i in posterior:\n",
        "    for j in i:\n",
        "      manipulated_posterior.append(j)\n",
        "  # Make an empty list numerators, that will contain all the X_ijmks multiplied by C_ij    \n",
        "  numerators = []\n",
        "  for ind_post, X_ijmk in zip(range(len(manipulated_posterior)), X_ijmks):\n",
        "    # Turn X_ijmk into an array\n",
        "    X_ijmk_array = np.array(X_ijmk)\n",
        "    # Multiply X_ijmk by 1 minus the posterior (C_ij)\n",
        "    numerator = X_ijmk_array * (1 - manipulated_posterior[ind_post])\n",
        "    # Add the multiplied window sequences to the list \"numerators\"\n",
        "    numerators.append(numerator)\n",
        "  numerator_before_normalization = np.array(numerators).sum(axis = 0)\n",
        "  new_psi0 = np.divide(numerator_before_normalization, (len(onehot_matrix)*(len(onehot_matrix[0]) - motif_length + 1 - 1)))\n",
        "  # Verify that new_psi0 rows sum to 1\n",
        "  print('The sums of each row in our new psi0 are:')\n",
        "  for i in range(len(new_psi0)):\n",
        "     print(sum(new_psi0[i]))\n",
        "  # Add new_psi1 to new_params\n",
        "  new_params[\"psi0\"] = new_psi0.tolist()\n",
        "  \n",
        "  # Save our new parameters as the output for the function\n",
        "  return new_params\n",
        "\n",
        "# Test the M-step\n",
        "m_step_test = M_step(my_ohe_sequence_padded, practice_posterior, motif_length)\n",
        "print(m_step_test['lambda_j'])\n",
        "print(m_step_test['psi1'])\n",
        "print(m_step_test['psi0'])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sum of our new lambda_j is: 0.9999999999999997\n",
            "The sums of each row in our new psi1 are:\n",
            "1.0\n",
            "1.0000000000000002\n",
            "1.0\n",
            "1.0\n",
            "0.9999999999999998\n",
            "0.9999999999999993\n",
            "1.0000000000000004\n",
            "1.0\n",
            "1.0000000000000002\n",
            "1.0000000000000002\n",
            "1.0000000000000002\n",
            "0.9999999999999993\n",
            "1.0000000000000002\n",
            "1.0\n",
            "1.0\n",
            "1.0000000000000002\n",
            "0.9999999999999999\n",
            "1.0000000000000002\n",
            "The sums of each row in our new psi0 are:\n",
            "1.0000000000000004\n",
            "1.0000000000000007\n",
            "1.0000000000000002\n",
            "1.0\n",
            "1.0000000000000004\n",
            "1.0000000000000004\n",
            "1.0000000000000004\n",
            "1.0000000000000004\n",
            "1.0000000000000007\n",
            "1.0000000000000009\n",
            "1.0000000000000007\n",
            "1.0000000000000004\n",
            "1.0000000000000009\n",
            "1.0000000000000007\n",
            "1.0000000000000009\n",
            "1.0000000000000009\n",
            "1.0000000000000004\n",
            "1.0000000000000002\n",
            "[0.047481147965896614, 0.04697197047788, 0.04605146854806403, 0.047807173734906096, 0.04782771124725615, 0.04441263628818571, 0.04756089606618445, 0.046121524520974726, 0.04804282910380134, 0.04654685482718043, 0.04713693091738038, 0.047495248364932824, 0.047190600375296925, 0.049979065997599034, 0.04661722304755457, 0.047660863019842147, 0.05011814043490774, 0.04809894062526165, 0.0486217255040825, 0.05158499415976082, 0.0466720547730516]\n",
            "[[0.25809579847970915, 0.23323670709285096, 0.2790080885641299, 0.22965940586331], [0.2526264485343683, 0.22675101241479953, 0.27105036978832875, 0.2495721692625036], [0.2418976175143708, 0.21911956449188327, 0.29504209879010473, 0.2439407192036411], [0.25070779644486263, 0.22267595413448613, 0.28893389469079817, 0.23768235472985294], [0.2478251552634219, 0.24860864423750398, 0.2729526405823102, 0.23061355991676366], [0.23789775079077993, 0.2701934162858368, 0.26426103956925273, 0.22764779335412988], [0.22578092371543673, 0.2727435768926706, 0.2560344708779523, 0.24544102851394076], [0.22211939171258444, 0.2771406683024217, 0.2613541547693628, 0.23938578521563114], [0.2280813523757467, 0.27245004497093867, 0.2624538454458466, 0.23701475720746826], [0.22329522506677066, 0.27661836309707494, 0.263842106654965, 0.23624430518118963], [0.22500677062068952, 0.2714826327277945, 0.2700903944002069, 0.2334202022513093], [0.23021925459168877, 0.26132600096928144, 0.2678097327746465, 0.2406450116643827], [0.2215874150349797, 0.2716634312596168, 0.2747016383842791, 0.2320475153211246], [0.216430969588003, 0.2740999920874709, 0.2645278403183264, 0.24494119800619968], [0.2202499878233857, 0.28940730388708097, 0.2443495289285433, 0.2459931793609899], [0.2276340015961987, 0.29639993848024737, 0.21175445562901488, 0.2642116042945393], [0.23581145777099796, 0.3069452050317335, 0.21952916836226602, 0.23771416883500243], [0.24851645582832113, 0.27295891754320434, 0.2327206488165627, 0.24580397781191196]]\n",
            "[[0.26020445377349355, 0.23287597977140803, 0.2793269064961634, 0.22759265995893554], [0.24955355152286138, 0.22605740736245367, 0.2713214310904158, 0.25306761002427], [0.24574825637918343, 0.22013645875019575, 0.2926708642481698, 0.24144442062245136], [0.2556718930909222, 0.22233959164901834, 0.2849930811758239, 0.23699543408423554], [0.2450317114245045, 0.24681326526711617, 0.2751478861781675, 0.23300713713021237], [0.23712472030359844, 0.27206455887758507, 0.2643779844361034, 0.22643273638271347], [0.22918714429041892, 0.27039643460074486, 0.25624589550372145, 0.2441705256051153], [0.22530859624070143, 0.2718572522991644, 0.2643832726536889, 0.23845087880644572], [0.22389005002827173, 0.27139150335369394, 0.26460840016468246, 0.24011004645335263], [0.22440946843853815, 0.2710430314249787, 0.2663597153955434, 0.23818778474094054], [0.22488411525047874, 0.2710197058986244, 0.26758791725477943, 0.23650826159611804], [0.22686438741047127, 0.26354434420923906, 0.27106329487387276, 0.23852797350641736], [0.22057329031267658, 0.2672291533669918, 0.27660105253456785, 0.23559650378576447], [0.20948657476989974, 0.2753706306477273, 0.26982682927259904, 0.2453159653097745], [0.2183992653147131, 0.28594980287287325, 0.24492538069643024, 0.25072555111598416], [0.22601325790338397, 0.2944237005549788, 0.21616297749866176, 0.2634000640429762], [0.23484808257363501, 0.3056611431097585, 0.22333726707208315, 0.2361535072445237], [0.24667781866516666, 0.2703856675682185, 0.2348625669989472, 0.2480739467676679]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4G4z6HyvUM1"
      },
      "source": [
        "Write a function to compute the **log likelihood**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQuPi4fJvS37"
      },
      "source": [
        "#mariele's rough first shot at log likelihood\n",
        "#relied heavlily on geralds code, so i think my ideas are right but instead we are going to need to use for loops to implement them \n",
        "#next try is going to go deeper into math, mostly hoping for something functional rn\n",
        "# compute log likelihood, given posteriors based off the current parameter set theta (so KL = 0)\n",
        "def loglikelihood(ohe_matrix, params, posteriors):\n",
        "  #weighted probabilities\n",
        "  weighted_log_joint_prob = (np.log(params['psi1'])+np.log(params['lambda_j']))*posteriors+((np.log(params['psi0'])+np.log(params['lambda_j']))*(1-posteriors))\n",
        "  weighted_log_joint_prob[np.where(posteriors == 0)]=0; #putting in a check in case some posteriors are 0 (because some psi are 0) - 0log0 = 0 (otherwise we will get nan)\n",
        "  \n",
        "  expected_complete_LL = np.sum(weighted_log_joint_prob.T*ohe_matrix)\n",
        "  #entropy \n",
        "  qlogq = posteriors * np.log(posteriors);\n",
        "  qlogq[np.where(posteriors == 0)] = 0 #0log0 = 0\n",
        "  return (expected_complete_LL - np.sum(np.sum(qlogq,axis=1) * ohe_matrix)) #log likelihood = ELBO + entropy, when q=p\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHLSgbKV1HuR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "1ffc9ae0-19fa-4680-a63a-419ec14cc91e"
      },
      "source": [
        "# Viki's attempt (don't want to delete Mariele's stuff)\n",
        "\n",
        "def loglikelihood_VH(onehot_matrix, params, posterior):\n",
        "  # Calculation for lambda_j term\n",
        "  num_js = len(onehot_matrix[0]) - motif_length + 1\n",
        "  for ind_i in range(len(onehot_matrix)):\n",
        "    for ind_j in range(num_js):\n",
        "      #print(posterior[ind_i][ind_j])\n",
        "      lambda_j_term = 0\n",
        "  print(len(posterior[0]))\n",
        "  print(len(params['lambda_j']))\n",
        "\n",
        "  # Calculation for psi1 term\n",
        "  psi1_term = 0\n",
        "\n",
        "  # Calculation for psi0 term\n",
        "  psi0_term = 0\n",
        "\n",
        "  # Calculate log likelihood\n",
        "  llh = lambda_j_term + psi1_term + psi0_term\n",
        "  # Return log likelihood\n",
        "  return llh\n",
        "\n",
        "\n",
        "\n",
        "# test it\n",
        "#posterior_matrix = E_step(params,seq_data,len(seq_data[0]),motif_length)\n",
        "#posterior_matrix\n",
        "test_llh = loglikelihood_VH(seq_data, params, posterior_matrix)\n",
        "print(test_llh)\n",
        "'''\n",
        "Scratch code:\n",
        "\n",
        "def loglikelihood_VH(onehot_matrix, params, posterior, motif_length):\n",
        "\tllh = 0.0\n",
        "\tfor ind_i in range(len(onehot_matrix)):\n",
        "\t\tfor ind_j in range(0, len(onehot_matrix[0]) - motif_length + 1):\n",
        "\t\t\tllh += posterior[i][j] * np.log(params['lambda_j'][ind_j])\n",
        "\t\t\tfor ind_p in range(motif_length):\n",
        "\t\t\t\tbase = onehot_matrix[ind_i][ind_j + ind_p]\n",
        "\t\t\t\tllh += posterior[ind_i][ind_j] * np.log(params['psi1'][base][ind_p])\n",
        "\t\t\t\tllh += (1 - posterior[ind_i][ind_j]) * np.log(params['psi0'][base][ind_p])\n",
        "\treturn llh\n",
        "\n",
        "\n",
        "\n",
        "  # Determine how many j positions we have in our input matrix\n",
        "  num_js = len(onehot_matrix[0]) - motif_length + 1\n",
        "  # Determine matrix dimensions and save our window sequences\n",
        "  X_ijmks = []\n",
        "  for i, ind_i in zip(onehot_matrix, range(len(onehot_matrix))):\n",
        "    # Iterate through the indeces for our lambda parameter\n",
        "    for ind_j in range(num_js):\n",
        "        # i[ind_j:ind_j+motif_length] is the base pair identities of the One-Hot encoded matrix within the window starting at j\n",
        "        X_ijmk = (i[ind_j:ind_j+motif_length]).tolist()\n",
        "        # Save each X_ijmk to the X_ijmks list for ease of access\n",
        "        X_ijmks.append(X_ijmk)\n",
        "  # Manipulate the posterior so it maches the X_ijmks list\n",
        "  manipulated_posterior = []\n",
        "  for i in posterior:\n",
        "    for j in i:\n",
        "      manipulated_posterior.append(j)\n",
        "  for ind_post, X_ijmk in zip(range(len(manipulated_posterior)), X_ijmks):\n",
        "    # Turn X_ijmk into an array\n",
        "    X_ijmk_array = np.array(X_ijmk)\n",
        "    # Multiply X_ijmk by 1 minus the posterior (C_ij)\n",
        "    numerator = X_ijmk_array * (1 - manipulated_posterior[ind_post])\n",
        "    # Add the multiplied window sequences to the list \"numerators\"\n",
        "    numerators.append(numerator)\n",
        "\n",
        "'''"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "21\n",
            "0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nScratch code:\\n\\ndef loglikelihood_VH(onehot_matrix, params, posterior, motif_length):\\n\\tllh = 0.0\\n\\tfor ind_i in range(len(onehot_matrix)):\\n\\t\\tfor ind_j in range(0, len(onehot_matrix[0]) - motif_length + 1):\\n\\t\\t\\tllh += posterior[i][j] * np.log(params[\\'lambda_j\\'][ind_j])\\n\\t\\t\\tfor ind_p in range(motif_length):\\n\\t\\t\\t\\tbase = onehot_matrix[ind_i][ind_j + ind_p]\\n\\t\\t\\t\\tllh += posterior[ind_i][ind_j] * np.log(params[\\'psi1\\'][base][ind_p])\\n\\t\\t\\t\\tllh += (1 - posterior[ind_i][ind_j]) * np.log(params[\\'psi0\\'][base][ind_p])\\n\\treturn llh\\n\\n\\n\\n  # Determine how many j positions we have in our input matrix\\n  num_js = len(onehot_matrix[0]) - motif_length + 1\\n  # Determine matrix dimensions and save our window sequences\\n  X_ijmks = []\\n  for i, ind_i in zip(onehot_matrix, range(len(onehot_matrix))):\\n    # Iterate through the indeces for our lambda parameter\\n    for ind_j in range(num_js):\\n        # i[ind_j:ind_j+motif_length] is the base pair identities of the One-Hot encoded matrix within the window starting at j\\n        X_ijmk = (i[ind_j:ind_j+motif_length]).tolist()\\n        # Save each X_ijmk to the X_ijmks list for ease of access\\n        X_ijmks.append(X_ijmk)\\n  # Manipulate the posterior so it maches the X_ijmks list\\n  manipulated_posterior = []\\n  for i in posterior:\\n    for j in i:\\n      manipulated_posterior.append(j)\\n  for ind_post, X_ijmk in zip(range(len(manipulated_posterior)), X_ijmks):\\n    # Turn X_ijmk into an array\\n    X_ijmk_array = np.array(X_ijmk)\\n    # Multiply X_ijmk by 1 minus the posterior (C_ij)\\n    numerator = X_ijmk_array * (1 - manipulated_posterior[ind_post])\\n    # Add the multiplied window sequences to the list \"numerators\"\\n    numerators.append(numerator)\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS8dXrNmsw2d"
      },
      "source": [
        "# **Run EM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9liLSAMwHEe"
      },
      "source": [
        "Set data file URL locations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "143x5QHNwKZg"
      },
      "source": [
        "# URL for at_gc_sequences.txt - this is a single sequence: ATTTAATATAAAATTTGGCCGCCATAAAAAAA\n",
        "at_gc_sequences_txt = 'https://ucdavis.box.com/shared/static/s8g6zx9vwxbbfdxdj2uqzhlvslc1jhsy.txt'\n",
        "# URL for sequence.padded.txt - the real binding site data\n",
        "sequence_padded_txt = 'https://ucdavis.box.com/shared/static/0cacx2xvn4ugxo9h21ci2ngesryigf43.txt'\n",
        "# URL for sequence.motiflocation.padded.txt - the location of the binding sites from sequence.padded.txt\n",
        "sequence_motiflocation_padded_txt = 'https://ucdavis.box.com/shared/static/gd0r12mdkhix86bo9ffbn3dy0fy0prmn.txt'"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H6PrulrH9ap"
      },
      "source": [
        "Load in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxoFtgmPH_Ev",
        "outputId": "9288ec0b-0652-4686-9e48-23ab2c6f3a16"
      },
      "source": [
        "# Implement get_sequence()\n",
        "# The variable my_ohe_sequence_padded corresponds to the one-hot encoded matrices that represent the sequence data\n",
        "# The variable my_sequence_padded_bp_counts is the base pair counts for the input sequence data\n",
        "# The variable my_sequence_padded_num_seqs returns the number of sequences in the sequence file\n",
        "# sequence_padded_txt is the variable name containing the URL defined at the beginning\n",
        "my_ohe_sequence_padded, my_sequence_padded_bp_counts = get_sequence(sequence_padded_txt, categories=['A', 'C', 'G', 'T'])\n",
        "\n",
        "# Following similar notation above, we can implement get_sequence() for the at_gc_sequences_text data\n",
        "my_ohe_at_gc_sequences, my_at_gc_bp_counts = get_sequence(at_gc_sequences_txt, categories=['A', 'C', 'G', 'T'])\n",
        "\n",
        "# Now randomly split the sequence_padded_txt into a training vs test set\n",
        "sequence_padded_train_bp_counts, sequence_padded_test_bp_counts = get_sequence_traintest(sequence_padded_txt, categories=['A', 'C', 'G', 'T'])\n",
        "\n",
        "# Get the locations of the motifs for the sequence.padded.txt file\n",
        "sequence_padded_motifs = pd.read_csv(io.StringIO(requests.get(sequence_motiflocation_padded_txt).text), sep=\",\", header=None).to_numpy()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 357/357 [00:00<00:00, 2349.61it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 939.58it/s]\n",
            "100%|██████████| 357/357 [00:00<00:00, 2483.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K12xuvJwvbMi"
      },
      "source": [
        "Assign EM run variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OBpGUmTvdri"
      },
      "source": [
        "# Length of the motif\n",
        "motif_length = 18\n",
        "\n",
        "# Sequence data used\n",
        "seq_data = my_ohe_sequence_padded\n",
        "\n",
        "# Set the number of sequences\n",
        "num_seqs = len(seq_data)\n",
        "\n",
        "# Set the length of the sequences\n",
        "seq_len = len(seq_data[0])\n",
        "\n",
        "# Set the number of EM iterations\n",
        "num_iterations = 20"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzlbCRTbFXVQ"
      },
      "source": [
        "Run the EM algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1af_j4kiOZti",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "3bc13ad3-b049-418a-f68b-32a807d332e0"
      },
      "source": [
        "# Create an empty list that will contain the log likelihoods for plotting later\n",
        "loglikelihoods = []\n",
        "\n",
        "# Run EM\n",
        "for i in range(1, num_iterations+1):\n",
        "  # Keep track of iteration number\n",
        "  print(f'Iteration #{i}')\n",
        "  if i == 1:\n",
        "    # Randomly initialize parameters for the first iteration\n",
        "    params = initialize_random_params(seq_len, motif_length)\n",
        "  else:\n",
        "    # Use parameters from the last EM implementation\n",
        "    params = new_params\n",
        "  # Run E-step to calculate posteriors\n",
        "  my_posterior = E_step(params,seq_data,seq_len,motif_length)\n",
        "  # Calculate log likelihood \n",
        "  llh = loglikelihood_VH(seq_data, params, my_posterior, motif_length)\n",
        "  # Add log likelihood to the loglikelihoods list \n",
        "  loglikelihoods.append(llh)\n",
        "  # View log likelihood after each iteration(we should see convergence)\n",
        "  print(llh)\n",
        "  # Run M-step to further optimize parameters using new posterior from E-step\n",
        "  new_params = M_step(seq_data, my_posterior, motif_length)\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Gerald's Code:\n",
        "np.random.seed(1)\n",
        "#XXss = XXss_at_gc\n",
        "XXss = sequences_padded_train\n",
        "theta = init_EM(NUM_MODELS = 3)\n",
        "\n",
        "#train\n",
        "for ii in range(3):  \n",
        "  posteriors = E_step(theta);  \n",
        "  print(loglikelihood(XXss, theta, posteriors))\n",
        "  theta = M_step(XXss, posteriors);\n",
        "\n",
        "#evaluate on held-out test data\n",
        "posteriors = E_step(theta)\n",
        "print('held-out likelihood:', loglikelihood(sequences_padded_test, theta, posteriors))\n",
        "'''\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration #1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-8d948f222042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mmy_posterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmotif_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Calculate log likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mllh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloglikelihood_VH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_posterior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotif_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;31m# Add log likelihood to the loglikelihoods list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mloglikelihoods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-d5597fbb8bfc>\u001b[0m in \u001b[0;36mloglikelihood_VH\u001b[0;34m(onehot_matrix, params, posterior, motif_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotif_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                 \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                 \u001b[0mllh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposterior\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'psi1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                                 \u001b[0mllh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mposterior\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'psi0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mllh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I1WRWsu6_V3"
      },
      "source": [
        "# **Run Experiments on EM Algorithm Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XNRby2I804q"
      },
      "source": [
        "Here, we will answer the questions to Assignment 1 Part 3 regarding our updated EM algorithm model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QATNfYSQ7uJS"
      },
      "source": [
        "## 1. Plot the log likelihood as a function of EM iteration, for 20 iterations, for 5 different random initializations of the model parameter. Does the log likelihood monotonically increase every iteration of every initialization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pRdAV1DKrW0"
      },
      "source": [
        "Start with fake data to ensure the plot works as intended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXPDC98w75PO"
      },
      "source": [
        "fake_rand_init_1 = [1, 2, 4, 7, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
        "fake_rand_init_2 = [1, 2, 3, 4, 5, 6, 8, 10, 10.5, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
        "fake_rand_init_3 = [0, 2, 4, 9, 10, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
        "fake_rand_init_4 = [1, 2, 4, 7, 10, 10, 10.1, 10.2, 10.3, 10.4, 10.5, 10.7, 11, 11, 11, 11, 11, 11, 11, 11]\n",
        "fake_rand_init_5 = [0, 1, 2, 8, 9, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGy4MFULRUtv"
      },
      "source": [
        "Make the graph of the log likelihood after each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxlLrJufRYTI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Set number of iterations\n",
        "iterations = 20\n",
        "\n",
        "# Create list containing iteration number (for x-axis labels)\n",
        "num_iterations = []\n",
        "# Use range (1, iterations+1) since Python uses 0-based indexing\n",
        "for i in range(1, iterations+1):\n",
        "  num_iterations.append(i)\n",
        "\n",
        "# Make a plot where the x-axis is the number of iterations\n",
        "# and the y-axis contains the log likelihoods\n",
        "plt.plot(num_iterations, fake_rand_init_1, label = \"fake_rand_init_1\")\n",
        "plt.plot(num_iterations, fake_rand_init_2, label = \"fake_rand_init_2\")\n",
        "plt.plot(num_iterations, fake_rand_init_3, label = \"fake_rand_init_3\")\n",
        "plt.plot(num_iterations, fake_rand_init_4, label = \"fake_rand_init_4\")\n",
        "plt.plot(num_iterations, fake_rand_init_5, label = \"fake_rand_init_5\")\n",
        "\n",
        "# Assign axis titles, plot titles, and legend\n",
        "plt.title('Log Likelihood as a Function of EM Iteration')\n",
        "plt.xlabel('Number of EM Iterations')\n",
        "plt.ylabel('Log Likelihood')\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "# Generate plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO67ygHC76X4"
      },
      "source": [
        "## 2. Draw a sequence logo visualization of the foreground motif your model learns, $\\psi^{l}_{m, k}$. You could try LogoMaker, a Python library (https://logomaker.readthedocs.io/en/latest/). Alternatively, there are a number of web servers for doing this; you could draw samples from your foreground model, and input those drawn sequences into e.g. the WebLogo server (https://weblogo.berkeley.edu)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU_x1Agb8ySl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpV2tMRA9I2k"
      },
      "source": [
        "## 3. Now run your model using model random initializations. How do the model parameters $\\psi^{l}_{m, k}$ compare across runs? What about their log likelihoods?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPxQA-BW9RBd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixs58dhD9WRZ"
      },
      "source": [
        "## 4. Plot a figure that shows the distribution over $C_{ij}$ for a few of the input sequences, and compare that (in the visualization) to the ground truth. How close was your model to predicting the real motif location?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEeQ51eY9cO_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHeUPqxu9nBr"
      },
      "source": [
        "## 5. Train your model using 80% of the data, holding out the remaining 20%. Evaluate the log likelihood of your held out data using the model you implemented in this assignment, and compare it to the log likelihood from the simple latent model we used in class, using the same training/held out data. Which one is better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWZ_T8zj9qMr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Ed0I_N9s9Z"
      },
      "source": [
        "## 6. Train your model on the atgcsequences.txt file (that had a GC-rich region embedded between two flanking AT-rich regions). Does the model work better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq80KKxQ-Ed3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkc5IO2o-E6i"
      },
      "source": [
        "## 7. The original training set in sequence.padded.txt has 357 sequences. Randomly sample another 357 sequences of the same length (just from a simple generator, that produces each base at equal frequency) and train the model with all data. Does it still recover the same motif? What if you add 3000 noisy sequences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EQJ9k8E-7T0"
      },
      "source": [
        "Generate 357 sequences of the same length that produces each base at equal frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMYZPCktUaAg"
      },
      "source": [
        "import random\n",
        "\n",
        "# Number of sequences to generate\n",
        "seqs = 357\n",
        "# The minimum sequence length\n",
        "min = 40\n",
        "# The maximum sequence length\n",
        "max = 40\n",
        "# GC-content as a proportion\n",
        "gc = 0.5\n",
        "\n",
        "assert(seqs > 0)\n",
        "assert(min > 0)\n",
        "assert(max >= min)\n",
        "assert(gc >= 0 and gc <= 1)\n",
        "\n",
        "# Use probabilities for random sequence generation\n",
        "# This does not guarantee that all bases are present at equal frequency\n",
        "# But this means the sequence length does not have to be divisible by 4\n",
        "# And base pair frequencies are close to equal\n",
        "my_seqs = []\n",
        "for i in range(seqs):\n",
        "    l = random.randint(min, max)\n",
        "    seq = []\n",
        "    for j in range(l):\n",
        "        r = random.random()\n",
        "        if r < gc:\n",
        "            r = random.random()\n",
        "            if r < 0.5: seq.append('G')\n",
        "            else:       seq.append('C')\n",
        "        else:\n",
        "            r = random.random()\n",
        "            if r < 0.5: seq.append('A')\n",
        "            else:       seq.append('T')\n",
        "    my_seqs.append(''.join(seq))\n",
        "\n",
        "print(my_seqs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}