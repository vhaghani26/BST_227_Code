{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vhaghani26/BST_227_Code/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEB6Jbmxr9P7"
      },
      "source": [
        "# **The EM Algorithm for a more complex sequence model**\n",
        "## By: Mariele Lensink and Viktoria Haghani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zDlf51Mr6wp"
      },
      "source": [
        "First, we need to import the modules we plan to use to run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yrbLTPNtux7"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tqdm import tqdm\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puqdz-Cxsg4i"
      },
      "source": [
        "\n",
        "# **Download and Process Sequence Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9liLSAMwHEe"
      },
      "source": [
        "Set data file URL locations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "143x5QHNwKZg"
      },
      "source": [
        "# URL for at_gc_sequences.txt - this is a single sequence: ATTTAATATAAAATTTGGCCGCCATAAAAAAA\n",
        "at_gc_sequences_txt = 'https://ucdavis.box.com/shared/static/s8g6zx9vwxbbfdxdj2uqzhlvslc1jhsy.txt'\n",
        "# URL for sequence.padded.txt - the real binding site data\n",
        "sequence_padded_txt = 'https://ucdavis.box.com/shared/static/0cacx2xvn4ugxo9h21ci2ngesryigf43.txt'\n",
        "# URL for sequence.motiflocation.padded.txt - the location of the binding sites from sequence.padded.txt\n",
        "sequence_motiflocation_padded_txt = 'https://ucdavis.box.com/shared/static/gd0r12mdkhix86bo9ffbn3dy0fy0prmn.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdktorRL4uEj"
      },
      "source": [
        "Write a function to download and one-hot encoded the sequence data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka8lwgcXZ5XF"
      },
      "source": [
        "# Function written by Chenxi Liu (slight modifications made)\n",
        "\n",
        "def get_sequence(url, categories=['A', 'C', 'G', 'T']):\n",
        "  # Send a GET request to a specified URL, categories=['A', 'C', 'G', 'T']):\n",
        "  r = requests.get(url)\n",
        "  # Convert sequence data to data frame\n",
        "  df = pd.read_csv(io.StringIO(r.text), sep=\" \", header=None)\n",
        "  # Turn the first sequence into a 2D array where each index is an independent array with a single base pair\n",
        "  s1 = np.array(list(str(df.to_numpy()[0, :][0])), dtype=object).reshape(-1, 1)\n",
        "\n",
        "  # Determine how many sequences there are in the text file\n",
        "  num_seqs = len(df)\n",
        "  # Assume all input sequences are equal length\n",
        "  # Determine how long each sequence is\n",
        "  seq_len = len(list(df.iloc[0, :].values)[0])\n",
        "  # Start to make a one-hot encoded 3D matrix for the seqeunces\n",
        "  # Not one-hot encoded yet, just zeroes \n",
        "  data = np.zeros((num_seqs, seq_len, len(categories)))\n",
        "  # Make a matrix repersenting each category ['A', 'C', 'G', 'T']\n",
        "  bp_counts = np.zeros((1, len(categories)))\n",
        "\n",
        "  # Encode categorical feautures in a one-hot numeric array\n",
        "  # Assign categories to be used\n",
        "  ohe = OneHotEncoder(sparse=False, categories=[np.array(categories, dtype=object)])\n",
        "  # Apply OneHotEncoder to example sequence\n",
        "  ohe.fit(s1)\n",
        "  \n",
        "  # Apply OneHotEncoder to all sequences\n",
        "  for ii in tqdm(range(num_seqs)):\n",
        "    s = list(str(df.to_numpy()[ii, :][0]))\n",
        "    s_a = np.array(s).reshape(-1, 1)\n",
        "    data[ii, :, :] = ohe.transform(s_a)\n",
        "  \n",
        "  # Count the number of each base in the sequence data\n",
        "  for ii in range(len(categories)):\n",
        "    bp_counts[0,ii] = np.sum(data[:,:,ii])\n",
        "  \n",
        "  # Return the one-hot encoded matrix (data),\n",
        "  # the counts for each base pair (bp_counts),\n",
        "  # the number of sequences (N),\n",
        "  # and the length of each sequence (L)\n",
        "  return data, bp_counts, num_seqs, seq_len                                                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od2WQCQrDDyv"
      },
      "source": [
        "Write a modified version of the above function that returns a randomly split training and test data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJGr38gKDNuE"
      },
      "source": [
        "# Function written by Chenxi Liu and Gerald Quon\n",
        "\n",
        "def get_sequence_traintest(url, categories=['A', 'C', 'G', 'T'], FRACTION_TRAINING=0.8):\n",
        "  # Send a GET request to a specified URL, categories=['A', 'C', 'G', 'T']):\n",
        "  r = requests.get(url)\n",
        "  # Convert sequence data to data frame\n",
        "  df = pd.read_csv(io.StringIO(r.text), sep=\" \", header=None)\n",
        "  # Turn the first sequence into a 2D array where each index is an independent array with a single base pair\n",
        "  s1 = np.array(list(str(df.to_numpy()[0, :][0])), dtype=object).reshape(-1, 1)\n",
        "\n",
        "  # Determine how many sequences there are in the text file\n",
        "  m = len(df)\n",
        "  # Assume all input sequences are equal length\n",
        "  # Determine how long each sequence is\n",
        "  sequence_len = len(list(df.iloc[0, :].values)[0])\n",
        "  # Start to make a one-hot encoded 3D matrix for the seqeunces\n",
        "  # Not one-hot encoded yet, just zeroes \n",
        "  data = np.zeros((m, sequence_len, len(categories)))\n",
        "  # Make a matrix repersenting each category ['A', 'C', 'G', 'T'] for the training set\n",
        "  bp_counts_train = np.zeros((1, len(categories)))\n",
        "  # Make a matrix repersenting each category ['A', 'C', 'G', 'T'] for the test set\n",
        "  bp_counts_test = np.zeros((1, len(categories)))\n",
        "\n",
        "  # Encode categorical feautures in a one-hot numeric array\n",
        "  # Assign categories to be used\n",
        "  ohe = OneHotEncoder(sparse=False, categories=[np.array(categories, dtype=object)])\n",
        "  # Apply OneHotEncoder to example sequence\n",
        "  ohe.fit(s1)\n",
        "\n",
        "  # Apply OneHotEncoder to all sequences\n",
        "  for ii in tqdm(range(m)):\n",
        "    s = list(str(df.to_numpy()[ii, :][0]))\n",
        "    s_a = np.array(s).reshape(-1, 1)\n",
        "    data[ii, :, :] = ohe.transform(s_a)\n",
        "\n",
        "  # Randomly permute rows of matrix\n",
        "  np.random.shuffle(data)\n",
        "\n",
        "  # Split data into training and text data\n",
        "  train_indices = np.arange(start=0,stop=round(FRACTION_TRAINING*data.shape[0]))\n",
        "  test_indices = np.arange(start=round(FRACTION_TRAINING*data.shape[0]), stop=data.shape[0])\n",
        "  \n",
        "  # Count the number of each base in the test and train sets\n",
        "  for ii in range(len(categories)):\n",
        "    bp_counts_train[0,ii] = np.sum(data[train_indices,:,ii])\n",
        "    bp_counts_test[0,ii] = np.sum(data[test_indices,:,ii])\n",
        "  \n",
        "  # Return the base pair counts for the test and train sets\n",
        "  return bp_counts_train, bp_counts_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H6PrulrH9ap"
      },
      "source": [
        "Load in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxoFtgmPH_Ev",
        "outputId": "0903931c-8726-4673-a5c6-cbafaadfc308"
      },
      "source": [
        "# Implement get_sequence()\n",
        "# The variable my_ohe_sequence_padded corresponds to the one-hot encoded matrices that represent the sequence data\n",
        "# The variable my_sequence_padded_bp_counts is the base pair counts for the input sequence data\n",
        "# The variable my_sequence_padded_num_seqs returns the number of sequences in the sequence file\n",
        "# sequence_padded_txt is the variable name containing the URL defined at the beginning\n",
        "my_ohe_sequence_padded, my_sequence_padded_bp_counts, my_sequence_padded_num_seqs, my_sequence_padded_seq_len = get_sequence(sequence_padded_txt, categories=['A', 'C', 'G', 'T'])\n",
        "\n",
        "# Following similar notation above, we can implement get_sequence() for the at_gc_sequences_text data\n",
        "my_ohe_at_gc_sequences, my_at_gc_bp_counts, my_at_gc_num_seqs, my_at_gc_seq_len = get_sequence(at_gc_sequences_txt, categories=['A', 'C', 'G', 'T'])\n",
        "\n",
        "# Now randomly split the sequence_padded_txt into a training vs test set\n",
        "sequence_padded_train_bp_counts, sequence_padded_test_bp_counts = get_sequence_traintest(sequence_padded_txt, categories=['A', 'C', 'G', 'T'])\n",
        "\n",
        "# Get the locations of the motifs for the sequence.padded.txt file\n",
        "sequence_padded_motifs = pd.read_csv(io.StringIO(requests.get(sequence_motiflocation_padded_txt).text), sep=\",\", header=None).to_numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 357/357 [00:00<00:00, 2390.58it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 121.41it/s]\n",
            "100%|██████████| 357/357 [00:00<00:00, 2290.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4CTnzFwsnzw"
      },
      "source": [
        "# **Core EM Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbAXV5YlM9p0"
      },
      "source": [
        "Write functions that will initialize our parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0uB6d0_M_b-"
      },
      "source": [
        "# Make a function to randomly generate L - P + 1 lambdas that sum to 1\n",
        "def initialize_lambda(sequence_length, motif_length):\n",
        "  # Make an empty list to represent our parameter lambda_j\n",
        "  lambda_j = []\n",
        "  # Given our sequence length and motif length, determine how many lambdas we need\n",
        "  lambdas = sequence_length - motif_length + 1\n",
        "  # Fill up our empty matrix with the right number of lambdas using random probabilities\n",
        "  for i in range(lambdas):\n",
        "    lambda_j.append(np.random.random_sample())\n",
        "  # Normalize probabilities by dividing each one by the sum\n",
        "  # Need to set sum_lambdas on the outside of the for loop so it isn't recalculated every time\n",
        "  sum_lambdas = sum(lambda_j)\n",
        "  for i, lmbda in zip(range(lambdas), lambda_j):\n",
        "    lambda_j[i] = (lmbda/sum_lambdas)\n",
        "  # Return our lambda_j parameter\n",
        "  return lambda_j\n",
        "\n",
        "# Make a function to randomly generate psi\n",
        "# Psi is a 2D matrix: 4 x P (P = motif-length) \n",
        "# Each set of 4 corresponds to ['A', 'C', 'G', 'T']\n",
        "def initialize_psi(motif_length):\n",
        "  # Make a 4 x P matrix of zeroes\n",
        "  psi = np.zeros((motif_length, 4), dtype = float)\n",
        "  # For each sub-matrix of 4, generate probabilities that sum to 1\n",
        "  for ind_i, i in zip(range(motif_length), psi):\n",
        "    psi[ind_i] = (np.random.dirichlet(np.ones(4),size=1))\n",
        "  # Return our psi parameter\n",
        "  return psi\n",
        "\n",
        "# Store all initialized parameters in a dictionary called 'params' (params = theta)\n",
        "def initialize_random_params(sequence_length, motif_length):\n",
        "    params = {'lambda_j': initialize_lambda(sequence_length, motif_length),\n",
        "              'psi0': initialize_psi(motif_length),\n",
        "              'psi1': initialize_psi(motif_length)\n",
        "              }\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC-Qv1XpvDeI"
      },
      "source": [
        "Write a function to calculate the posterior for the **E-step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQxY3F0qvG_O"
      },
      "source": [
        "def E_step(params,ohe_matrix,sequence_length,motif_length):\n",
        "  posts = np.zeros((len(ohe_matrix),(sequence_length-motif_length)))\n",
        " #loop through every seqence in file \n",
        "  for i in range(len(ohe_matrix)):\n",
        "    #loop through \n",
        "    for j in range(sequence_length - motif_length):\n",
        "     #initialize C_ij \n",
        "      C_ij = np.log(params['lambda_j'][j])\n",
        "      #compute P(X_ij|Cij = 1)\n",
        "      for p in range(motif_length):\n",
        "        for k in range(4):\n",
        "          if ohe_matrix[i][j+p][k] != 0:\n",
        "            C_ij = C_ij + np.log(ohe_matrix[i][j+p][k]*params['psi1'][p][k])\n",
        "            posts[i][j] = C_ij\n",
        "      for jprime in range(sequence_length-motif_length):\n",
        "        if jprime == j:\n",
        "          continue \n",
        "        for p in range(motif_length):\n",
        "          for k in range(4):\n",
        "            if ohe_matrix[i][j+p][k] != 0:\n",
        "              C_ij = C_ij + np.log(ohe_matrix[i][j+p][k]*params['psi0'][p][k])\n",
        "              posts[i][j] = C_ij\n",
        "  posts1 = np.zeros((len(posts),len(posts[0])))\n",
        "  for i in range(len(posts)):\n",
        "    rowmin = posts[i].min()\n",
        "    for j in range(len(posts[i])):\n",
        "      posts1[i][j] = math.exp(posts[i][j] - rowmin)\n",
        "      #posts1[i][j] = exp(posts1[i][j])\n",
        "    rowsum = posts1[i].sum()\n",
        "    for j in range(len(posts1[i])):\n",
        "      posts1[i][j] = posts1[i][j]/rowsum\n",
        "\n",
        "\n",
        "  return posts1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Clr0AJjp1lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c921e86-338e-4fa2-8589-623c93de6277"
      },
      "source": [
        "posterior_matrix = E_step(params,my_ohe_at_gc_sequences,my_at_gc_seq_len,motif_length)\n",
        "posterior_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.03869068e-67, 3.36764134e-29, 1.18426916e-17, 2.76450857e-17,\n",
              "        5.02912726e-01, 4.62671285e-09, 4.46249036e-08, 1.42081792e-01,\n",
              "        3.55005433e-01, 4.06941191e-31, 1.09893676e-53, 1.55436543e-10,\n",
              "        5.09749774e-36, 4.55784559e-28]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S05xOBVOvHvh"
      },
      "source": [
        "Write a function to do the calculations for the **M-step**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8xF_OTs58bA"
      },
      "source": [
        "We will start with making a practice posterior of random probabilities. This allows us to troubleshoot the M-step regardless independently of the E-step. It does not get used in the EM algorithm implementation, but it's here for debugging purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m60-pmemrATK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400b5283-202d-40fe-f915-2975fe7928eb"
      },
      "source": [
        "# Make practice posterior to troubleshoot M-step\n",
        "\n",
        "# Set the number of sequences\n",
        "num_seqs = my_sequence_padded_num_seqs\n",
        "# Set the length of the sequences\n",
        "seq_len = my_sequence_padded_seq_len\n",
        "\n",
        "practice_posterior = []\n",
        "for i in range(num_seqs):\n",
        "  new_row = initialize_lambda(seq_len, motif_length)\n",
        "  practice_posterior.append(new_row)\n",
        "\n",
        "print(len(practice_posterior)) # Should be total number of sequences; 357 for sequence padded\n",
        "print(len(practice_posterior[0])) # Should be number of j's; 31 for sequence padded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "357\n",
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGa58p1LvRHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bb70c7-e580-4017-c7b2-f0405a66400d"
      },
      "source": [
        "def M_step(onehot_matrix, posterior, motif_length): \n",
        "  # Make dictionary to store new parameters\n",
        "  new_params = {}\n",
        "\n",
        "  # Calculate new lambda_j\n",
        "  # Add every column of the posterior\n",
        "  column_sums = np.array(posterior).sum(axis = 0)\n",
        "  # Divide by the total number of sequences\n",
        "  new_lambda_j = (column_sums/(len(posterior))).tolist()\n",
        "  # Confirm that the sum of our new_lambda_j is 1\n",
        "  print(f'The sum of our new lambda_j is: {sum(new_lambda_j)}')\n",
        "  # Add new_lambda_j to new_params\n",
        "  new_params[\"lambda_j\"] = new_lambda_j\n",
        "\n",
        "  # Calculate new psi1\n",
        "  # Determine how many j positions we have in our input matrix\n",
        "  num_js = len(onehot_matrix[0]) - motif_length + 1\n",
        "  # Determine matrix dimensions and save our window sequences\n",
        "  X_ijmks = []\n",
        "  for i, ind_i in zip(onehot_matrix, range(len(onehot_matrix))):\n",
        "    # Iterate through the indeces for our lambda parameter\n",
        "    for ind_j in range(num_js):\n",
        "        # i[ind_j:ind_j+motif_length] is the base pair identities of the One-Hot encoded matrix within the window starting at j\n",
        "        X_ijmk = (i[ind_j:ind_j+motif_length]).tolist()\n",
        "        # Save each X_ijmk to the X_ijmks list for ease of access\n",
        "        X_ijmks.append(X_ijmk)\n",
        "  # Manipulate the posterior so it maches the X_ijmks list\n",
        "  manipulated_posterior = []\n",
        "  for i in posterior:\n",
        "    for j in i:\n",
        "      manipulated_posterior.append(j)\n",
        "  # Make an empty list numerators, that will contain all the X_ijmks multiplied by C_ij    \n",
        "  numerators = []\n",
        "  for ind_post, X_ijmk in zip(range(len(manipulated_posterior)), X_ijmks):\n",
        "    # Turn X_ijmk into an array\n",
        "    X_ijmk_array = np.array(X_ijmk)\n",
        "    # Multiply X_ijmk by the posterior (C_ij)\n",
        "    numerator = X_ijmk_array * manipulated_posterior[ind_post]\n",
        "    # Add the multiplied window sequences to the list \"numerators\"\n",
        "    numerators.append(numerator)\n",
        "  numerator_before_normalization = np.array(numerators).sum(axis = 0)\n",
        "  new_psi1 = np.divide(numerator_before_normalization, len(onehot_matrix))\n",
        "  # Verify that new_psi1 rows sum to 1\n",
        "  print('The sums of each row in our new psi1 are:')\n",
        "  for i in range(len(new_psi1)):\n",
        "     print(sum(new_psi1[i]))\n",
        "  # Add new_psi1 to new_params\n",
        "  new_params[\"psi1\"] = new_psi1.tolist()\n",
        "\n",
        "  # Calculate new psi0\n",
        "  # Determine how many j positions we have in our input matrix\n",
        "  num_js = len(onehot_matrix[0]) - motif_length + 1\n",
        "  # Determine matrix dimensions and save our window sequences\n",
        "  X_ijmks = []\n",
        "  for i, ind_i in zip(onehot_matrix, range(len(onehot_matrix))):\n",
        "    # Iterate through the indeces for our lambda parameter\n",
        "    for ind_j in range(num_js):\n",
        "        # i[ind_j:ind_j+motif_length] is the base pair identities of the One-Hot encoded matrix within the window starting at j\n",
        "        X_ijmk = (i[ind_j:ind_j+motif_length]).tolist()\n",
        "        # Save each X_ijmk to the X_ijmks list for ease of access\n",
        "        X_ijmks.append(X_ijmk)\n",
        "  # Manipulate the posterior so it maches the X_ijmks list\n",
        "  manipulated_posterior = []\n",
        "  for i in posterior:\n",
        "    for j in i:\n",
        "      manipulated_posterior.append(j)\n",
        "  # Make an empty list numerators, that will contain all the X_ijmks multiplied by C_ij    \n",
        "  numerators = []\n",
        "  for ind_post, X_ijmk in zip(range(len(manipulated_posterior)), X_ijmks):\n",
        "    # Turn X_ijmk into an array\n",
        "    X_ijmk_array = np.array(X_ijmk)\n",
        "    # Multiply X_ijmk by 1 minus the posterior (C_ij)\n",
        "    numerator = X_ijmk_array * (1 - manipulated_posterior[ind_post])\n",
        "    # Add the multiplied window sequences to the list \"numerators\"\n",
        "    numerators.append(numerator)\n",
        "  numerator_before_normalization = np.array(numerators).sum(axis = 0)\n",
        "  new_psi0 = np.divide(numerator_before_normalization, (len(onehot_matrix)*(len(onehot_matrix[0]) - motif_length + 1 - 1)))\n",
        "  # Verify that new_psi0 rows sum to 1\n",
        "  print('The sums of each row in our new psi0 are:')\n",
        "  for i in range(len(new_psi0)):\n",
        "     print(sum(new_psi0[i]))\n",
        "  # Add new_psi1 to new_params\n",
        "  new_params[\"psi0\"] = new_psi0.tolist()\n",
        "  \n",
        "  # Save our new parameters as the output for the function\n",
        "  return new_params\n",
        "\n",
        "# Test the M-step\n",
        "m_step_test = M_step(my_ohe_sequence_padded, practice_posterior, motif_length)\n",
        "print(m_step_test['lambda_j'])\n",
        "print(m_step_test['psi1'])\n",
        "print(m_step_test['psi0'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sum of our new lambda_j is: 1.0\n",
            "The sums of each row in our new psi1 are:\n",
            "0.9999999999999991\n",
            "0.9999999999999997\n",
            "0.9999999999999998\n",
            "0.9999999999999996\n",
            "0.9999999999999996\n",
            "0.9999999999999994\n",
            "0.9999999999999996\n",
            "0.9999999999999999\n",
            "0.9999999999999993\n",
            "0.9999999999999996\n",
            "0.9999999999999994\n",
            "0.9999999999999996\n",
            "0.9999999999999991\n",
            "0.9999999999999997\n",
            "0.9999999999999991\n",
            "0.999999999999999\n",
            "0.9999999999999998\n",
            "0.9999999999999996\n",
            "The sums of each row in our new psi0 are:\n",
            "0.999999999999999\n",
            "0.999999999999999\n",
            "0.9999999999999989\n",
            "0.9999999999999993\n",
            "0.9999999999999993\n",
            "0.9999999999999991\n",
            "0.9999999999999989\n",
            "0.9999999999999993\n",
            "0.9999999999999991\n",
            "0.999999999999999\n",
            "0.9999999999999993\n",
            "0.9999999999999993\n",
            "0.9999999999999996\n",
            "0.9999999999999991\n",
            "0.9999999999999992\n",
            "0.9999999999999991\n",
            "0.9999999999999984\n",
            "0.9999999999999993\n",
            "[0.047444631482734145, 0.048121470705003985, 0.047927088685222184, 0.045821661797577094, 0.04990583181970714, 0.04682858001903071, 0.047160346680440536, 0.04820774897400453, 0.051379868986518196, 0.04799632821586307, 0.04811882924282198, 0.04614312612082798, 0.04682830170792582, 0.0482578321374906, 0.046930241398383726, 0.04823268668783575, 0.04582319109483804, 0.04816118129454417, 0.04474195866241837, 0.049348222075185556, 0.04662087221162648]\n",
            "[[0.2620742040210397, 0.2322693356960603, 0.27434569298598127, 0.23131076729691788], [0.2469173473467331, 0.230120051555053, 0.26931619418852587, 0.25364640690968765], [0.25050160776963654, 0.21592824609575986, 0.2894181379702431, 0.2441520081643604], [0.2544964996315729, 0.22186867246734054, 0.2911910791301409, 0.23244374877094515], [0.25103692785439374, 0.24125350226520947, 0.27294733334573074, 0.2347622365346655], [0.23217025866540353, 0.26954182325242954, 0.27136357226687785, 0.22692434581528842], [0.2295549841997708, 0.26710049825789167, 0.25684667884196155, 0.2464978387003755], [0.2254980283321186, 0.2664912691578121, 0.2662789401948201, 0.24173176231524907], [0.22546829839972846, 0.27436140040227985, 0.2638988622607795, 0.23627143893721145], [0.2245397616547127, 0.27159345067205065, 0.2680020284109308, 0.23586475926230538], [0.22436971355584498, 0.2681314239540097, 0.2709294939285774, 0.2365693685615674], [0.22746219089466815, 0.26415202944854305, 0.27118624040414563, 0.23719953925264275], [0.2183416133959664, 0.27122059916158925, 0.276957642266539, 0.2334801451759046], [0.20754865025868635, 0.2710786456833006, 0.26790976587193155, 0.25346293818608123], [0.2168226658634892, 0.2877813719804408, 0.2454232879797841, 0.249972674176285], [0.22977722062155168, 0.29011950375365514, 0.21557151084096698, 0.2645317647838251], [0.2379101574672431, 0.3063641772377854, 0.22111904611616406, 0.23460661917880718], [0.2422553176949935, 0.27620877560072915, 0.23041226870246215, 0.25112363800181486]]\n",
            "[[0.2600055334964271, 0.232924348341247, 0.27956002627507026, 0.2275100918872547], [0.2498390065822426, 0.2258889554054404, 0.27140813987040563, 0.25286389814191035], [0.24531805686641994, 0.2202960246700018, 0.29295206228916254, 0.24143385617441465], [0.2554824579315866, 0.22237995573237518, 0.2848802219538566, 0.2372573643821809], [0.24487112279495532, 0.24718102236573145, 0.27514815153999556, 0.23279970329931693], [0.23741109490986714, 0.27209713852925527, 0.2640228578012213, 0.22646890875965536], [0.2289984412662018, 0.2706785885324831, 0.25620528510552054, 0.24411768509579349], [0.22513966440972388, 0.27238972225639513, 0.2641370333824155, 0.2383335799514647], [0.22402070272707264, 0.2712959355821262, 0.2645361493239355, 0.24014721236686482], [0.22434724160914082, 0.271294277046229, 0.2661517193077448, 0.2382067620368844], [0.22491596810372014, 0.2711872663373136, 0.26754596227836025, 0.23635080328060534], [0.22700224059532245, 0.263403042785276, 0.2708944694923972, 0.23870024712700366], [0.22073558039462718, 0.2672512949718926, 0.2764882523404539, 0.23552487229302582], [0.20993069073636536, 0.2755216979679357, 0.2696577329949182, 0.24488987830077974], [0.21857063141270738, 0.2860310994682045, 0.2448716927438678, 0.2505265763752196], [0.2259060969521155, 0.2947377222913084, 0.21597212473806332, 0.2633840560185119], [0.23474314758882286, 0.30569019449945395, 0.22325777318438783, 0.23630888472733377], [0.24699087557183275, 0.2702231746653415, 0.23497798600465297, 0.2478079637581721]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4G4z6HyvUM1"
      },
      "source": [
        "Compute the **log likelihood**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQuPi4fJvS37"
      },
      "source": [
        "#mariele's rough first shot at log likelihood\n",
        "#relied heavlily on geralds code, so i think my ideas are right but instead we are going to need to use for loops to implement them \n",
        "#next try is going to go deeper into math, mostly hoping for something functional rn\n",
        "# compute log likelihood, given posteriors based off the current parameter set theta (so KL = 0)\n",
        "def loglikelihood(ohe_matrix, params, posteriors):\n",
        "  #weighted probabilities\n",
        "  weighted_log_joint_prob = (np.log(params['psi1'])+np.log(params['lambda_j']))*posteriors+((np.log(params['psi0'])+np.log(params['lambda_j']))*(1-posteriors))\n",
        "  weighted_log_joint_prob[np.where(posteriors == 0)]=0; #putting in a check in case some posteriors are 0 (because some psi are 0) - 0log0 = 0 (otherwise we will get nan)\n",
        "  \n",
        "  expected_complete_LL = np.sum(weighted_log_joint_prob.T*ohe_matrix)\n",
        "  #entropy \n",
        "  qlogq = posteriors * np.log(posteriors);\n",
        "  qlogq[np.where(posteriors == 0)] = 0 #0log0 = 0\n",
        "  return (expected_complete_LL - np.sum(np.sum(qlogq,axis=1) * ohe_matrix)) #log likelihood = ELBO + entropy, when q=p\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS8dXrNmsw2d"
      },
      "source": [
        "# **Run EM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzlbCRTbFXVQ"
      },
      "source": [
        "Run the EM algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1af_j4kiOZti",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8354722-7725-4962-fb2c-78d19a8e7a18"
      },
      "source": [
        "# Assign variables:\n",
        "# Length of the motif\n",
        "motif_length = 18\n",
        "# Sequence data used\n",
        "seq_data = my_ohe_sequence_padded\n",
        "# Set the number of sequences\n",
        "num_seqs = len(seq_data)\n",
        "# Set the length of the sequences\n",
        "seq_len = len(seq_data[0])\n",
        "# Set the number of EM iterations\n",
        "num_iterations = 20\n",
        "\n",
        "# Run EM\n",
        "for i in range(1, num_iterations+1):\n",
        "  if i == 1:\n",
        "    # Randomly initialize parameters for the first iteration\n",
        "    params = initialize_random_params(seq_len, motif_length)\n",
        "  else:\n",
        "    # Use parameters from the last EM implementation\n",
        "    params = new_params\n",
        "  my_posterior = E_step(params,seq_data,seq_len,motif_length)\n",
        "  new_params = M_step(seq_data, my_posterior, motif_length)\n",
        "  llh = loglikelihood(seq_data, params, my_posterior)\n",
        "  print(f'Iteration #{i}')\n",
        "  print(llh)\n",
        "\n",
        "\n",
        "'''\n",
        "Gerald's Code:\n",
        "np.random.seed(1)\n",
        "#XXss = XXss_at_gc\n",
        "XXss = sequences_padded_train\n",
        "theta = init_EM(NUM_MODELS = 3)\n",
        "\n",
        "#train\n",
        "for ii in range(3):  \n",
        "  posteriors = E_step(theta);  \n",
        "  print(loglikelihood(XXss, theta, posteriors))\n",
        "  theta = M_step(XXss, posteriors);\n",
        "\n",
        "#evaluate on held-out test data\n",
        "posteriors = E_step(theta)\n",
        "print('held-out likelihood:', loglikelihood(sequences_padded_test, theta, posteriors))\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sum of our new lambda_j is: -12778.982453414059\n",
            "The sums of each row in our new psi1 are:\n",
            "-12778.982453414063\n",
            "-12778.982453414059\n",
            "-12778.982453414055\n",
            "-12778.982453414063\n",
            "-12778.982453414063\n",
            "-12778.98245341406\n",
            "-12778.982453414059\n",
            "-12778.982453414063\n",
            "-12778.982453414064\n",
            "-12778.98245341406\n",
            "-12778.982453414063\n",
            "-12778.982453414053\n",
            "-12778.98245341406\n",
            "-12778.982453414064\n",
            "-12778.982453414057\n",
            "-12778.982453414064\n",
            "-12778.982453414057\n",
            "-12778.982453414066\n",
            "The sums of each row in our new psi0 are:\n",
            "639.9491226707031\n",
            "639.949122670703\n",
            "639.9491226707028\n",
            "639.9491226707031\n",
            "639.9491226707032\n",
            "639.949122670703\n",
            "639.949122670703\n",
            "639.9491226707031\n",
            "639.9491226707033\n",
            "639.9491226707031\n",
            "639.9491226707031\n",
            "639.9491226707026\n",
            "639.949122670703\n",
            "639.9491226707032\n",
            "639.9491226707029\n",
            "639.9491226707032\n",
            "639.949122670703\n",
            "639.9491226707032\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-e803b1bf042f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mmy_posterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmotif_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mnew_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_posterior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotif_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mllh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloglikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_posterior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Iteration #{i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-52c8c1492efa>\u001b[0m in \u001b[0;36mloglikelihood\u001b[0;34m(ohe_matrix, params, posteriors)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloglikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mohe_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposteriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#weighted probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mweighted_log_joint_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'psi1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda_j'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mposteriors\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'psi0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda_j'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mposteriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mweighted_log_joint_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposteriors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m#putting in a check in case some posteriors are 0 (because some psi are 0) - 0log0 = 0 (otherwise we will get nan)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (18,4) (21,) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I1WRWsu6_V3"
      },
      "source": [
        "# **Run Experiments on EM Algorithm Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XNRby2I804q"
      },
      "source": [
        "Here, we will answer the questions to Assignment 1 Part 3 regarding our updated EM algorithm model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QATNfYSQ7uJS"
      },
      "source": [
        "## 1. Plot the log likelihood as a function of EM iteration, for 20 iterations, for 5 different random initializations of the model parameter. Does the log likelihood monotonically increase every iteration of every initialization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pRdAV1DKrW0"
      },
      "source": [
        "Start with fake data to ensure the plot works as intended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXPDC98w75PO"
      },
      "source": [
        "fake_rand_init_1 = [1, 2, 4, 7, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
        "fake_rand_init_2 = [1, 2, 3, 4, 5, 6, 8, 10, 10.5, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n",
        "fake_rand_init_3 = [0, 2, 4, 9, 10, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
        "fake_rand_init_4 = [1, 2, 4, 7, 10, 10, 10.1, 10.2, 10.3, 10.4, 10.5, 10.7, 11, 11, 11, 11, 11, 11, 11, 11]\n",
        "fake_rand_init_5 = [0, 1, 2, 8, 9, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGy4MFULRUtv"
      },
      "source": [
        "Make the graph of the log likelihood after each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxlLrJufRYTI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iterations = 20\n",
        "\n",
        "# Create list containing iteration number (x-axis)\n",
        "num_iterations = []\n",
        "# Use range (1, iterations+1) since Python uses 0-based indexing\n",
        "for i in range(1, iterations+1):\n",
        "  num_iterations.append(i)\n",
        "\n",
        "# Make a plot where the x-axis is the number of iterations\n",
        "# and the y-axis contains the log likelihoods\n",
        "plt.plot(num_iterations, fake_rand_init_1)\n",
        "plt.plot(num_iterations, fake_rand_init_2)\n",
        "plt.plot(num_iterations, fake_rand_init_3)\n",
        "plt.plot(num_iterations, fake_rand_init_4)\n",
        "plt.plot(num_iterations, fake_rand_init_5)\n",
        "\n",
        "# Assign axis and plot titles\n",
        "plt.title('Log Likelihood as a Function of EM Iteration')\n",
        "plt.xlabel('Number of EM Iterations')\n",
        "plt.ylabel('Log Likelihood')\n",
        "\n",
        "# Generate plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO67ygHC76X4"
      },
      "source": [
        "## 2. Draw a sequence logo visualization of the foreground motif your model learns, $\\psi^{l}_{m, k}$. You could try LogoMaker, a Python library (https://logomaker.readthedocs.io/en/latest/). Alternatively, there are a number of web servers for doing this; you could draw samples from your foreground model, and input those drawn sequences into e.g. the WebLogo server (https://weblogo.berkeley.edu)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU_x1Agb8ySl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpV2tMRA9I2k"
      },
      "source": [
        "## 3. Now run your model using model random initializations. How do the model parameters $\\psi^{l}_{m, k}$ compare across runs? What about their log likelihoods?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPxQA-BW9RBd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixs58dhD9WRZ"
      },
      "source": [
        "## 4. Plot a figure that shows the distribution over $C_{ij}$ for a few of the input sequences, and compare that (in the visualization) to the ground truth. How close was your model to predicting the real motif location?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEeQ51eY9cO_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHeUPqxu9nBr"
      },
      "source": [
        "## 5. Train your model using 80% of the data, holding out the remaining 20%. Evaluate the log likelihood of your held out data using the model you implemented in this assignment, and compare it to the log likelihood from the simple latent model we used in class, using the same training/held out data. Which one is better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWZ_T8zj9qMr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Ed0I_N9s9Z"
      },
      "source": [
        "## 6. Train your model on the atgcsequences.txt file (that had a GC-rich region embedded between two flanking AT-rich regions). Does the model work better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq80KKxQ-Ed3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkc5IO2o-E6i"
      },
      "source": [
        "## 7. The original training set in sequence.padded.txt has 357 sequences. Randomly sample another 357 sequences of the same length (just from a simple generator, that produces each base at equal frequency) and train the model with all data. Does it still recover the same motif? What if you add 3000 noisy sequences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EQJ9k8E-7T0"
      },
      "source": [
        "Generate 357 sequences of the same length that produces each base at equal frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMYZPCktUaAg"
      },
      "source": [
        "import random\n",
        "\n",
        "# Number of sequences to generate\n",
        "seqs = 357\n",
        "# The minimum sequence length\n",
        "min = 40\n",
        "# The maximum sequence length\n",
        "max = 40\n",
        "# GC-content as a proportion\n",
        "gc = 0.5\n",
        "\n",
        "assert(seqs > 0)\n",
        "assert(min > 0)\n",
        "assert(max >= min)\n",
        "assert(gc >= 0 and gc <= 1)\n",
        "\n",
        "# Use probabilities for random sequence generation\n",
        "# This does not guarantee that all bases are present at equal frequency\n",
        "# But this means the sequence length does not have to be divisible by 4\n",
        "# And base pair frequencies are close to equal\n",
        "my_seqs = []\n",
        "for i in range(seqs):\n",
        "    l = random.randint(min, max)\n",
        "    seq = []\n",
        "    for j in range(l):\n",
        "        r = random.random()\n",
        "        if r < gc:\n",
        "            r = random.random()\n",
        "            if r < 0.5: seq.append('G')\n",
        "            else:       seq.append('C')\n",
        "        else:\n",
        "            r = random.random()\n",
        "            if r < 0.5: seq.append('A')\n",
        "            else:       seq.append('T')\n",
        "    my_seqs.append(''.join(seq))\n",
        "\n",
        "print(my_seqs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}